{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Import dependencies"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "from calendar import monthrange\n",
            "import datetime\n",
            "import math\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import os\n",
            "import pandas as pd\n",
            "from sklearn.base import BaseEstimator\n",
            "from sklearn.cluster import FeatureAgglomeration\n",
            "from sklearn.compose import ColumnTransformer, make_column_selector\n",
            "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
            "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_regression, mutual_info_regression\n",
            "from sklearn.impute import SimpleImputer\n",
            "from sklearn.inspection import permutation_importance\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import root_mean_squared_error\n",
            "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Load training data frame\n",
            "Load the data frame for training, which also contains LAID_UP_TIMEs."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_excel(\"data/Vehicles_export_prices_scaled_train_eng.xlsx\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Split train and test set\n",
            "Split the loaded data frame into a separate train and test data set."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Data frame preparation\n",
            "This notebook uses two steps for preprocessing.\n",
            "One general preparation for data frames and one preprocessing encapsulated in a scikit-learn pipeline.\n",
            "Everything, what was impossible or too hard to implement in a scikit-learn pipeline, is done in the first preparation.\n",
            "\n",
            "The following subsections contain the functions for the general preparation."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Manual dtype correction\n",
            "We can not be sure that pandas imports the data frame with the correct data types.\n",
            "Because of that, we specify the data types of each data frame column manually.\n",
            "We also remove columns, which are obviously not important for the prediction of LAID_UP_TIMEs."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "dtypes = {\n",
            "    \"date\": \"datetime64[ns]\",\n",
            "    \"num\": np.float64,\n",
            "    \"str\": pd.StringDtype(),\n",
            "}\n",
            "\n",
            "columns_of_interest = {\n",
            "    # \"RPAKREP_VEHICLE_HKEY\": \"str\", # Some kind of unique car ID\n",
            "    \"COMPANY\": \"str\",\n",
            "    \"OFFICE\": \"str\",\n",
            "    \"OFFICE_MAIN_BRAND\": \"str\",\n",
            "    \"CHASSIS_NUMBER\": \"str\",\n",
            "    \"MANUFACTURER_SHORT\": \"str\",\n",
            "    \"MANUFACTURER\": \"str\",\n",
            "    \"VEHICLE_GROUP\": \"str\",\n",
            "    \"VEHICLE_TYPE\": \"str\",\n",
            "    \"MODEL_CODE\": \"str\",\n",
            "    \"VARIANT\": \"str\",\n",
            "    \"MILEAGE\": \"num\",\n",
            "    # \"OPERATING_HOURS\": \"num\", # Contains mostly 0s (only about 13 rows with other number values)\n",
            "    # \"MILAGE_IN_FIELD\": \"num\", # Contains only 1s\n",
            "    \"MILAGE_SALES\": \"num\",\n",
            "    # \"OPERATING_HOURS_SALES\": \"num\", # Same justification as OPERATING_HOURS\n",
            "    # \"RIM_KEY\": \"str\", # Contains only around 18 rows with non-empty values\n",
            "    \"COLOR_CODE\": \"str\",\n",
            "    \"COLOR_CODE_NAME\": \"str\",\n",
            "    \"COLOR\": \"str\",\n",
            "    \"COLOR_TYPE\": \"str\",\n",
            "    \"UPHOLSTERY_CODE\": \"str\",\n",
            "    \"UPHOLSTERY\": \"str\",\n",
            "    \"UPHOLSTERY_CODE_ALT\": \"str\",\n",
            "    # \"CERTIFICATE_TYPE\": \"str\", # Same justification as OPERATING_HOURS\n",
            "    # \"CERTIFICATE_TYPE_DATE\": \"num\", # Contains only 0s\n",
            "    \"FACTORY_NUMBER\": \"str\",\n",
            "    \"ENGINE_ID\": \"str\",\n",
            "    \"ENGINE_TYPE\": \"str\",\n",
            "    \"ENGINE_ID_ALT\": \"str\",\n",
            "    \"TRANSMISSION\": \"str\",\n",
            "    \"TRANSMISSION_TYPE\": \"str\",\n",
            "    \"TRANSMISSION_ID\": \"str\",\n",
            "    \"TRANSMISSION_SHORT\": \"str\",\n",
            "    \"TRANSMISSION_NAME\": \"str\",\n",
            "    \"RIMS\": \"str\",\n",
            "    \"FRONT_TIRES\": \"str\",\n",
            "    # \"FRONT_TIRES_CONDITION\": \"num\", # Contains only 0s\n",
            "    \"REAR_TIRES\": \"str\",\n",
            "    # \"REAR_TIRES_CONDITION\": \"num\", # Contains only 0s\n",
            "    \"NUMBER_DOORS\": \"num\",\n",
            "    \"NUMBER_SEATS\": \"num\",\n",
            "    \"PERMITTED_TOTAL_WEIGHT\": \"num\",\n",
            "    \"MAX_TRAILOR_LOAD\": \"num\",\n",
            "    \"CURB_WEIGHT\": \"num\",\n",
            "    \"YEAR_CONSTRUCTION\": \"num\",\n",
            "    \"CONSTRUCTION_MONTH\": \"num\",\n",
            "    \"NUMBER_AXLE\": \"num\",\n",
            "    \"NUMBER_ENGINE_CYLINDER\": \"num\",\n",
            "    # \"REPAIR_RKZ\": \"num\", # Contains only 0s\n",
            "    # \"OPTICAL_CONDITION\": \"num\", # Contains only 0s and one row with a 3\n",
            "    # \"TECHNICAL_CONDITION\": \"num\", # Contains only 0s\n",
            "    \"ACCIDENT_VEHICLE\": \"str\",\n",
            "    # \"COMMISSION_NUMBER\": \"str\", # Contains too much unique values\n",
            "    \"HORSEPOWER\": \"num\",\n",
            "    \"KW\": \"num\",\n",
            "    \"CCM\": \"num\",\n",
            "    \"NUMBER_OWNERS\": \"num\",\n",
            "    \"IS_USED_CAR\": \"num\",\n",
            "    \"LEASING_CONTRACT_DATE\": \"date\",\n",
            "    \"LEASING_START\": \"date\",\n",
            "    \"LEASING_END\": \"date\",\n",
            "    \"LEASING_MILAGE\": \"num\",\n",
            "    # \"PAINT_TYPE\": \"str\", # Contains only empty cells\n",
            "    \"FINANCING_TYPE\": \"str\",\n",
            "    \"FINANCING_TYPE_NAME\": \"str\",\n",
            "    \"KAT_VEHICLE\": \"str\",\n",
            "    \"FUEL_TYPE\": \"str\",\n",
            "    \"FUEL_TYPE_NAME\": \"str\",\n",
            "    \"DRIVE_TYPE\": \"str\",\n",
            "    \"DRIVE_TYPE_NAME\": \"str\",\n",
            "    \"VEHICLE_MODEL_ID\": \"str\",\n",
            "    \"VEHICLE_MODEL_ID_NAME\": \"str\",\n",
            "    \"COMMISSION_TYPE\": \"str\",\n",
            "    \"COMMISSION_TYPE_NAME\": \"str\",\n",
            "    \"DEMONSTRATION_STATUS\": \"str\",\n",
            "    \"PURCHASE_DATE\": \"date\",\n",
            "    \"PURCHASE_BOOKING_DATE\": \"date\",\n",
            "    \"PURCHASE_MILAGE\": \"num\",\n",
            "    # \"PURCHASE_OPERATION_HOURS\": \"num\", # Contains only 0s\n",
            "    \"PRICE_LIST\": \"num\",\n",
            "    # \"DAY_OF_REGISTRATION\": \"str\", # Contains only 'N's\n",
            "    \"AT_LOCATION_SINCE\": \"num\",\n",
            "    \"LAID_UP_TIME\": \"num\",\n",
            "    \"SOLD_CUSTOMER_ID\": \"str\",\n",
            "    \"SOLD_INVOICE_COSTUMER_ID\": \"str\",\n",
            "    \"MILAGE_SALE\": \"num\",\n",
            "    # \"OPERATION_HOURS_SALE\": \"num\", # Same justification as OPERATING_HOURS\n",
            "    # \"SOLD_INVOICE_COSTUMER_ID2\": \"str\", # Is nearly identical to SOLD_INVOICE_COSTUMER_ID\n",
            "    \"CUSTOMER_TYPE\": \"str\",\n",
            "    # \"CUSTOMER_GROUP\": \"str\", # Is nearly identical to CUSTOMER_FEATURE\n",
            "    # \"CUSTOMER_GROUP_NAME\": \"str\", # Is nearly identical to CUSTOMER_FEATURE_NAME\n",
            "    \"CUSTOMER_FEATURE\": \"str\",\n",
            "    \"CUSTOMER_FEATURE_NAME\": \"str\",\n",
            "    # \"SALE_CUSTOMER_ID2\": \"str\", # Is nearly identical to SOLD_CUSTOMER_ID\n",
            "    # \"CUSTOMER_SALE_GROUP\": \"str\", # Is nearly identical to CUSTOMER_SALE_GROUP2\n",
            "    # \"CUSTOMER_SALE_GROUP_NAME\": \"str\", # Is nearly identical to CUSTOMER_SALE_GROUP2_NAME (semantically)\n",
            "    \"CUSTOMER_SALE_GROUP2\": \"str\",\n",
            "    \"CUSTOMER_SALE_GROUP2_NAME\": \"str\",\n",
            "    \"SCALED_CURRENT_VALUE\": \"num\",\n",
            "    \"SCALED_INVENTURAL_VALUE\": \"num\",\n",
            "    \"SCALED_REPORT_VALUE\": \"num\",\n",
            "    # \"SCALED_VALUATION_PRICE\": \"num\", # Contains only 0s\n",
            "    \"SCALED_GUIDE_PRICE\": \"num\",\n",
            "    \"SCALED_TOTAL_SALES_PRICE_BASIS\": \"num\",\n",
            "    \"SCALED_TOTAL_SALE_PRICE\": \"num\",\n",
            "}\n",
            "\n",
            "\n",
            "def correct_df_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
            "    return df[columns_of_interest.keys()]\\\n",
            "        .astype({k: dtypes[v] for k, v in columns_of_interest.items()})"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Map specific features values\n",
            "Different values of string (categorical) features are merged together, if they mean the same.\n",
            "Also unsensical feature values are mapped to NULL-values."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Merge color values\n",
            "The COLOR feature contains over 4500 unique values.\n",
            "Many color values can be merged to a generic color name like 'blue'.\n",
            "The car color might be very important for the LAID_UP_TIME.\n",
            "Because of that we try to map the different color names to a limited set of generic color names."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "color_mapping = {\n",
            "    \"white\": [\"WeiÃŸ\", \"weiß\", \"weiÃƒ?\", \"weiss\", \"white\", \"bianco\", \"blanco\", \"pearl\", \"snow\"],\n",
            "    \"silver\": [\"silber\", \"silver\", \"magnetic sil\", \"tridion\", \"hightechsilb\"],\n",
            "    \"black\": [\"schwar\", \"blac\", \"obsidian\", \"nera\"],\n",
            "    \"orange\": [\"orange\"],\n",
            "    \"blue\": [\"blau\", \"blue\", \"bleu\", \"tansanitbl\"],\n",
            "    \"red\": [\"rot\", \"red\", \"rosso\", \"rojo\", \"tokio fusion\", \"tokyo fusion\", \"peperoncino\", \"bordeaux\"],\n",
            "    \"violet\": [\"violet\", \"lila\", \"magenta\", \"pink\"],\n",
            "    \"green\": [\"grün\", \"grÃ¼n\", \"green\", \"gruen\", \"gr}n\", \"camouflage\"],\n",
            "    \"yellow\": [\"gelb\", \"yellow\", \"sunflower\"],\n",
            "    \"brown\": [\"braun\", \"brown\", \"urban khaki\"],\n",
            "    \"bronze\": [\"bronze\"],\n",
            "    \"gold\": [\"gold\", \"bright dusk\", \"platinum quartz\"],\n",
            "    \"beige\": [\"wheat\", \"beige\", \"elfenbein\", \"ivory\", \"sand\"],\n",
            "    \"brass\": [\"brass\", \"messing\"],\n",
            "    \"grey\": [\"grau\", \"gray\", \"grey\", \"storm bay\", \"anthrazit\", \"thunder\", \"nero\", \"granit\", \"magnetic\"],\n",
            "}\n",
            "\n",
            "\n",
            "def get_color_mapping(color):\n",
            "    for k in color_mapping.keys():\n",
            "        for v in color_mapping[k]:\n",
            "            if v.lower() in color.lower():\n",
            "                return k\n",
            "    return \"unknown\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Merge other features values\n",
            "The values of other features can also be merged, like 'MERCEDES' and 'MERCEDESBENZ'.\n",
            "There are also some typos in the values like 'CIRROEN' instead of 'CITROEN' which are also merged.\n",
            "Some feature values can simply be interpreted as 'not available'."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Value for NULL-strings\n",
            "str_na = pd.NA\n",
            "\n",
            "MANUFACTURER_mappings = {\n",
            "    str_na: [\"ALLGEMEIN\", \"ALLGEMEINSONSTIGE\", \"FABRIKAT\", \"FREMDFABRIKAT\", \"FREMDTEILENURFÃ¼RTEILE\", \"FREMDTEILFREMDFABRIKAT\", \"MUSSZUGETEILTWERDEN\"],\n",
            "    \"CITROEN\": [\"CIRROEN\", \"CITROÃªN\"],\n",
            "    \"MERCEDES\": [\"MERCEDESBENZ\"],\n",
            "    \"VW\": [\"VOLKSWAGEN\"],\n",
            "}\n",
            "\n",
            "COLOR_CODE_NAME_mappings = {\n",
            "    str_na: [\"NICHTDEFINIERT\"] # German for \"not definied\"\n",
            "}\n",
            "\n",
            "ENGINE_TYPE_mappings = {\n",
            "    str_na: [\"BITTEMOTIDPFLEGEN\"] # German for \"please maintain motor-ID\"\n",
            "}\n",
            "\n",
            "TRANSMISSION_NAME_mappings = {\n",
            "    str_na: [\"NICHTDEFINIERT\"] # German for \"not definied\"\n",
            "}\n",
            "\n",
            "FINANCING_TYPE_NAME_mappings = {\n",
            "    str_na: [\"NICHTDEFINIERT\"] # German for \"not definied\"\n",
            "}\n",
            "\n",
            "FUEL_TYPE_NAME_mappings = {\n",
            "    str_na: [\"ANDERE\"] # German for \"others\"\n",
            "}\n",
            "\n",
            "DRIVE_TYPE_NAME_mappings = {\n",
            "    str_na: [\"NICHTDEFINIERT\", \"ANDERE\"] # German for \"not definied\" and \"other\"\n",
            "}\n",
            "\n",
            "VEHICLE_MODEL_ID_NAME_mappings = {\n",
            "    str_na: [\"KEINEZUTEILUNG\"] # German for \"not definied\"\n",
            "}\n",
            "\n",
            "generic_code_mappings = {\n",
            "    str_na: [\"N\"] # Should stand for a \"not available\"\n",
            "}\n",
            "\n",
            "\n",
            "# Maps all values of a feature, which are contained in the values-list to NaN.\n",
            "def map_to_nan(feature: pd.DataFrame, values: list) -> pd.DataFrame:\n",
            "    return feature.map(lambda x: np.nan if x in values else x)\n",
            "\n",
            "\n",
            "# Normalizes the values of a string-feature.\n",
            "# This is done to merge similar string features automatically to some degree.\n",
            "def normalize_str_column(feature: pd.DataFrame) -> pd.DataFrame:\n",
            "    return feature\\\n",
            "        .map(lambda x: pd.NA if type(x) != str else remove_special_characters(x.upper()))\\\n",
            "        .map(lambda x: pd.NA if type(x) != str or not x else x)\\\n",
            "        .astype(dtypes[\"str\"])\n",
            "\n",
            "\n",
            "# Removes all non alpha-numeric characters from a string.\n",
            "def remove_special_characters(s: str) -> str:\n",
            "    return \"\".join(filter(str.isalnum, s))\n",
            "\n",
            "\n",
            "# Applys a mapping for string feature values to the feature.\n",
            "def map_str_column(feature: pd.DataFrame, mappings: object) -> pd.DataFrame:\n",
            "    return feature.map(lambda x: pd.NA if type(x) != str else map_str(x, mappings)).astype(dtypes[\"str\"])\n",
            "\n",
            "\n",
            "# Maps a string to one of the values in the mappings object.\n",
            "# If the string is not contained in the mappings it is returned as is.\n",
            "def map_str(s: str, mappings: object) -> str:\n",
            "    if not s:\n",
            "        return str_na\n",
            "    for key in mappings:\n",
            "        if s in mappings[key]:\n",
            "            return key\n",
            "    return s\n",
            "\n",
            "\n",
            "def map_specific_feature_values(df: pd.DataFrame) -> pd.DataFrame:\n",
            "    df_map = df.copy()\n",
            "\n",
            "    # Date features\n",
            "\n",
            "    # Years in the context of leasing above 2050 seemed unusual.\n",
            "    df_map[\"LEASING_CONTRACT_DATE\"] = df_map[\"LEASING_CONTRACT_DATE\"].map(lambda x: pd.NaT if x.year > 2050 else x)\n",
            "    df_map[\"LEASING_END\"] = df_map[\"LEASING_END\"].map(lambda x: pd.NaT if x.year > 2050 else x)\n",
            "\n",
            "    # Num features\n",
            "\n",
            "    # The following features should contain values > 0 for cars => 0s are mapped to NaN.\n",
            "    df_map[\"PERMITTED_TOTAL_WEIGHT\"] = map_to_nan(df_map[\"PERMITTED_TOTAL_WEIGHT\"], [0.0])\n",
            "    df_map[\"CURB_WEIGHT\"] = map_to_nan(df_map[\"CURB_WEIGHT\"], [0.0])\n",
            "    df_map[\"HORSEPOWER\"] = map_to_nan(df_map[\"HORSEPOWER\"], [0.0])\n",
            "    df_map[\"KW\"] = map_to_nan(df_map[\"KW\"], [0.0])\n",
            "    df_map[\"CCM\"] = map_to_nan(df_map[\"CCM\"], [0.0])\n",
            "    df_map[\"SCALED_INVENTURAL_VALUE\"] = map_to_nan(df_map[\"SCALED_INVENTURAL_VALUE\"], [0.0])\n",
            "    # Cars with a construction year befor 1900 seemed not realistic.\n",
            "    df_map[\"YEAR_CONSTRUCTION\"] = df_map[\"YEAR_CONSTRUCTION\"].map(lambda x: np.nan if x < 1900 else x)\n",
            "\n",
            "    # Str features\n",
            "\n",
            "    # The following mappings were created by manually looking the feature values up and looking for abnormalities.\n",
            "    df_map[\"MANUFACTURER_SHORT\"] = normalize_str_column(df_map[\"MANUFACTURER_SHORT\"])\n",
            "    df_map[\"MANUFACTURER\"] = map_str_column(normalize_str_column(df_map[\"MANUFACTURER\"]), MANUFACTURER_mappings)\n",
            "    df_map[\"VEHICLE_GROUP\"] = normalize_str_column(df_map[\"VEHICLE_GROUP\"])\n",
            "    df_map[\"VEHICLE_TYPE\"] = normalize_str_column(df_map[\"VEHICLE_TYPE\"])\n",
            "    df_map[\"MODEL_CODE\"] = normalize_str_column(df_map[\"MODEL_CODE\"])\n",
            "    df_map[\"COLOR_CODE_NAME\"] = map_str_column(normalize_str_column(df_map[\"COLOR_CODE_NAME\"]), COLOR_CODE_NAME_mappings)\n",
            "    df_map[\"COLOR\"] = normalize_str_column(df_map[\"COLOR\"])\n",
            "    df_map[\"COLOR_TYPE\"] = map_str_column(df_map[\"COLOR_TYPE\"], generic_code_mappings)\n",
            "    df_map[\"UPHOLSTERY\"] = normalize_str_column(df_map[\"UPHOLSTERY\"])\n",
            "    df_map[\"UPHOLSTERY_CODE_ALT\"] = map_str_column(df_map[\"UPHOLSTERY_CODE_ALT\"], generic_code_mappings)\n",
            "    df_map[\"FACTORY_NUMBER\"] = normalize_str_column(df_map[\"FACTORY_NUMBER\"])\n",
            "    df_map[\"ENGINE_TYPE\"] = map_str_column(normalize_str_column(df_map[\"ENGINE_TYPE\"]), ENGINE_TYPE_mappings)\n",
            "    df_map[\"ENGINE_ID_ALT\"] = normalize_str_column(df_map[\"ENGINE_ID_ALT\"])\n",
            "    df_map[\"TRANSMISSION_TYPE\"] = normalize_str_column(df_map[\"TRANSMISSION_TYPE\"])\n",
            "    df_map[\"TRANSMISSION_ID\"] = normalize_str_column(df_map[\"TRANSMISSION_ID\"])\n",
            "    df_map[\"TRANSMISSION_NAME\"] = map_str_column(normalize_str_column(df_map[\"TRANSMISSION_NAME\"]), TRANSMISSION_NAME_mappings)\n",
            "    df_map[\"ACCIDENT_VEHICLE\"] = map_str_column(df_map[\"ACCIDENT_VEHICLE\"], generic_code_mappings)\n",
            "    df_map[\"FINANCING_TYPE\"] = map_str_column(df_map[\"FINANCING_TYPE\"], generic_code_mappings)\n",
            "    df_map[\"FINANCING_TYPE_NAME\"] = map_str_column(normalize_str_column(df_map[\"FINANCING_TYPE_NAME\"]), FINANCING_TYPE_NAME_mappings)\n",
            "    df_map[\"KAT_VEHICLE\"] = map_str_column(df_map[\"KAT_VEHICLE\"], generic_code_mappings)\n",
            "    df_map[\"FUEL_TYPE\"] = map_str_column(df_map[\"FUEL_TYPE\"], generic_code_mappings)\n",
            "    df_map[\"FUEL_TYPE_NAME\"] = map_str_column(normalize_str_column(df_map[\"FUEL_TYPE_NAME\"]), FUEL_TYPE_NAME_mappings)\n",
            "    df_map[\"DRIVE_TYPE\"] = map_str_column(df_map[\"DRIVE_TYPE\"], generic_code_mappings)\n",
            "    df_map[\"DRIVE_TYPE_NAME\"] = map_str_column(normalize_str_column(df_map[\"DRIVE_TYPE_NAME\"]), DRIVE_TYPE_NAME_mappings)\n",
            "    df_map[\"VEHICLE_MODEL_ID_NAME\"] = map_str_column(normalize_str_column(df_map[\"VEHICLE_MODEL_ID_NAME\"]), VEHICLE_MODEL_ID_NAME_mappings)\n",
            "    df_map[\"CUSTOMER_FEATURE_NAME\"] = normalize_str_column(df_map[\"CUSTOMER_FEATURE_NAME\"])\n",
            "    df_map[\"CUSTOMER_SALE_GROUP2_NAME\"] = normalize_str_column(df_map[\"CUSTOMER_SALE_GROUP2_NAME\"])\n",
            "\n",
            "    # Map colors to either a base color or pd.NA\n",
            "    df_map[\"COLOR\"] = df_map[\"COLOR\"]\\\n",
            "        .map(lambda x: pd.NA if type(x) != str else get_color_mapping(x))\\\n",
            "        .map(lambda x: pd.NA if type(x) != str or x == \"unknown\" else x)\\\n",
            "        .astype(dtypes[\"str\"])\n",
            "\n",
            "    return df_map"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Add custom features\n",
            "We construct some custom features from other ones."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
            "\n",
            "months = [\n",
            "    \"January\",\n",
            "    \"February\",\n",
            "    \"March\",\n",
            "    \"April\",\n",
            "    \"May\",\n",
            "    \"June\",\n",
            "    \"July\",\n",
            "    \"August\",\n",
            "    \"September\",\n",
            "    \"October\",\n",
            "    \"November\",\n",
            "    \"December\"\n",
            "]\n",
            "\n",
            "\n",
            "# Creates a date from year, month and day.\n",
            "def create_date(year, month=1, day=1):\n",
            "    if math.isnan(year) or math.isnan(month) or math.isnan(day):\n",
            "        return pd.NaT\n",
            "    if year < 1900:\n",
            "        return pd.NaT\n",
            "    return datetime.datetime(int(year), int(month), int(day))\n",
            "\n",
            "\n",
            "# Maps an one-based month index to its name.\n",
            "def month_idx_to_name(month_idx):\n",
            "    if math.isnan(month_idx):\n",
            "        return pd.NA\n",
            "    return months[int(month_idx - 1)]\n",
            "\n",
            "\n",
            "# Maps a day number to a string.\n",
            "def day_idx_to_name(day_idx):\n",
            "    if math.isnan(day_idx):\n",
            "        return pd.NA\n",
            "    return int(day_idx)\n",
            "\n",
            "\n",
            "# Maps an zero-based weekday index to its name.\n",
            "def weekday_idx_to_name(weekday_idx):\n",
            "    if math.isnan(weekday_idx):\n",
            "        return pd.NA\n",
            "    return days[int(weekday_idx)]\n",
            "\n",
            "\n",
            "def add_custom_features(df: pd.DataFrame) -> pd.DataFrame:\n",
            "    df_feat = df.copy()\n",
            "    \n",
            "    # Compare OFFICE_MAIN_BRAND and MANUFACTURER_SHORT (create binary feature)\n",
            "    df_feat[\"COMP__OFFICE_MAIN_BRAND__MANUFACTURER_SHORT\"] = \\\n",
            "        (df_feat[\"OFFICE_MAIN_BRAND\"] == df_feat[\"MANUFACTURER_SHORT\"])\\\n",
            "        .astype(dtypes[\"num\"])\n",
            "    \n",
            "    # Compare SOLD_CUSTOMER_ID and SOLD_INVOICE_COSTUMER_ID (create binary feature)\n",
            "    df_feat[\"COMP__SOLD_CUSTOMER_ID__SOLD_INVOICE_COSTUMER_ID\"] = \\\n",
            "        (df_feat[\"SOLD_CUSTOMER_ID\"] == df_feat[\"SOLD_INVOICE_COSTUMER_ID\"])\\\n",
            "        .astype(dtypes[\"num\"])\n",
            "    \n",
            "    # Compare CUSTOMER_FEATURE and CUSTOMER_SALE_GROUP2 (create binary feature)\n",
            "    df_feat[\"COMP__CUSTOMER_FEATURE__CUSTOMER_SALE_GROUP2\"] = \\\n",
            "        (df_feat[\"CUSTOMER_FEATURE\"] == df_feat[\"CUSTOMER_SALE_GROUP2\"])\\\n",
            "        .astype(dtypes[\"num\"])\n",
            "\n",
            "    # Construct datetime from YEAR_CONSTRUCTION and CONSTRUCTION_MONTH\n",
            "    df_feat[\"CONSTRUCTION_DATE\"] = df_feat[[\"YEAR_CONSTRUCTION\", \"CONSTRUCTION_MONTH\"]]\\\n",
            "        .apply(lambda x: create_date(year=x[\"YEAR_CONSTRUCTION\"], month=x[\"CONSTRUCTION_MONTH\"]), axis=1)\\\n",
            "        .astype(dtypes[\"date\"])\n",
            "\n",
            "    # Separate year, month, day, weekday for date features\n",
            "    for col in df_feat.select_dtypes(include=dtypes[\"date\"]).columns:\n",
            "        df_feat[f\"{col}__YEAR\"] = df_feat[col].map(lambda date: date.year)\\\n",
            "            .astype(dtypes[\"num\"])\n",
            "\n",
            "        df_feat[f\"{col}__MONTH\"] = df_feat[col].map(lambda date: month_idx_to_name(date.month))\\\n",
            "            .astype(dtypes[\"str\"])\n",
            "\n",
            "        df_feat[f\"{col}__DAY\"] = df_feat[col].map(lambda date: day_idx_to_name(date.day))\\\n",
            "            .astype(dtypes[\"str\"])\n",
            "        \n",
            "        df_feat[f\"{col}__WEEKDAY\"] = df_feat[col].map(lambda date: weekday_idx_to_name(date.weekday()))\\\n",
            "            .astype(dtypes[\"str\"])\n",
            "    \n",
            "    # Create features from time spans between different date features\n",
            "    date_cols = df_feat.select_dtypes(include=dtypes[\"date\"]).columns\n",
            "    for date_col_idx_a in range(len(date_cols)):\n",
            "        for date_col_idx_b in range(date_col_idx_a + 1, len(date_cols)):\n",
            "            date_col_a = date_cols[date_col_idx_a]\n",
            "            date_col_b = date_cols[date_col_idx_b]\n",
            "            df_feat[f\"DIFF__{date_col_a}__{date_col_b}\"] = \\\n",
            "                (df_feat[date_col_a] - df_feat[date_col_b])\\\n",
            "                .map(lambda diff: diff.days)\\\n",
            "                .astype(dtypes[\"num\"])\n",
            "    \n",
            "    return df_feat"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Percentile clamping\n",
            "Some numeric features contain outliers.\n",
            "To remove them we compute the 1-percentile and 99-percentile for every numeric feature and clamp the feature values in that range."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "cols_to_ignore = [\n",
            "    \"LAID_UP_TIME\"\n",
            "]\n",
            "\n",
            "# List to save the computed percentiles\n",
            "perc_qs = []\n",
            "\n",
            "\n",
            "def clamp_percentiles(df: pd.DataFrame, fit: bool) -> pd.DataFrame:\n",
            "    df_perc = df.copy()\n",
            "\n",
            "    if fit:\n",
            "        perc_qs.clear()\n",
            "\n",
            "    for col in df_perc.select_dtypes(include=dtypes[\"num\"]).columns:\n",
            "        # Ignore features which contain integers (none continuous numeric features)\n",
            "        if len(df_perc[col].unique()) < 50:\n",
            "            continue\n",
            "        if col in cols_to_ignore:\n",
            "            continue\n",
            "\n",
            "        # 'scaled' features should be in the range [0, 1]\n",
            "        if col.startswith(\"SCALED_\"):\n",
            "            df_perc[col] = df_perc[col].map(lambda x: 0.0 if x < 0.0 else (1.0 if x > 1.0 else x))\n",
            "            continue\n",
            "        \n",
            "        q_low = 0.0\n",
            "        q_high = 0.0\n",
            "        if fit:\n",
            "            q_low = df_perc[col].quantile(0.01)\n",
            "            q_high = df_perc[col].quantile(0.99)\n",
            "            perc_qs.append([col, q_low, q_high])\n",
            "        else:\n",
            "            perc_q = next(x for x in perc_qs if x[0] == col)\n",
            "            q_low = perc_q[1]\n",
            "            q_high = perc_q[2]\n",
            "\n",
            "        df_perc[col] = df_perc[col].map(lambda x: q_low if x < q_low else (q_high if x > q_high else x))\n",
            "    return df_perc"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Preparation function\n",
            "The final function for general data frame preparation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "def filter_non_num_values(feature: pd.DataFrame) -> pd.DataFrame:\n",
            "    return feature.map(lambda x: x if type(x) == float or type(x) == int else np.nan)\n",
            "\n",
            "\n",
            "def prepare_data_frame(df: pd.DataFrame, is_test: bool, drop_rows: bool) -> pd.DataFrame:\n",
            "    df_prep = df.copy()\n",
            "\n",
            "    # YEAR_CONSTRUCTION Fix\n",
            "    df_prep[\"YEAR_CONSTRUCTION\"] = filter_non_num_values(df_prep[\"YEAR_CONSTRUCTION\"])\n",
            "\n",
            "    # PRICE_LIST Fix\n",
            "    df_prep[\"PRICE_LIST\"] = filter_non_num_values(df_prep[\"PRICE_LIST\"])\n",
            "    \n",
            "    # Manual dtype correction\n",
            "    df_prep = correct_df_dtypes(df_prep)\n",
            "\n",
            "    # Keep only unique rows\n",
            "    if drop_rows:\n",
            "        df_prep = df_prep.drop_duplicates()\n",
            "\n",
            "    # Map specific feature values\n",
            "    df_prep = map_specific_feature_values(df_prep)\n",
            "    \n",
            "    # Add custom features\n",
            "    df_prep = add_custom_features(df_prep)\n",
            "\n",
            "    # Percentile filtering\n",
            "    df_prep = clamp_percentiles(df_prep, not is_test)\n",
            "    \n",
            "    if drop_rows:\n",
            "        # Clamp LAID_UP_TIMEs, which are to high\n",
            "        if not is_test:\n",
            "            df_prep[\"LAID_UP_TIME\"] = df_prep[\"LAID_UP_TIME\"].map(lambda x: 3000 if x > 3000 else x)\n",
            "    \n",
            "        # Drop train samples without label value\n",
            "        df_prep = df_prep.dropna(subset=[\"LAID_UP_TIME\"])\n",
            "\n",
            "    return df_prep"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare loaded data frame\n",
            "The training data frame and the test data frame are prepared with the previous steps."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_train_prep = prepare_data_frame(df_train, is_test=False, drop_rows=True)\n",
            "df_test_prep = prepare_data_frame(df_test, is_test=True, drop_rows=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_train_prep.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_train_prep.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Inspect features\n",
            "Inspect the features of the prepared data frame."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Inspect the distribution of numeric features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_train_prep.hist(bins=64, figsize=(60, 40))\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Inspect numeric feature correlation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corr_matrix = df_train_prep.select_dtypes(include=dtypes[\"num\"]).corr()\n",
            "print(corr_matrix[\"LAID_UP_TIME\"][corr_matrix[\"LAID_UP_TIME\"].notna()].sort_values(ascending=False))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Split features and labels\n",
            "Split the data frames into features and labels."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "def split_data_frame(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
            "    # Features\n",
            "    X = df.drop(\"LAID_UP_TIME\", axis=1)\n",
            "\n",
            "    # Labels\n",
            "    y = df[\"LAID_UP_TIME\"]\n",
            "\n",
            "    return X, y"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train, y_train = split_data_frame(df_train_prep)\n",
            "X_test, y_test = split_data_frame(df_test_prep)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Inspect distribution of LAID_UP_TIME"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(f\"mean: {np.mean(y_train)}\")\n",
            "print(f\"std: {np.std(y_train)}\")\n",
            "plt.hist(y_train, bins=256)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Preprocessing\n",
            "This section contains the second preprocessing encapsulated in a scikit-learn pipeline."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Pipeline creation\n",
            "The function `create_preprocessor` create the preprocessing pipeline."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [],
         "source": [
            "def map_date(feature: pd.DataFrame) -> pd.DataFrame:\n",
            "    return feature.map(lambda date: date_to_number(date))\n",
            "\n",
            "\n",
            "# Converts a date to a float. The ordering of dates is kept in the float representation.\n",
            "def date_to_number(date: np.datetime64) -> np.float64:\n",
            "    if date is pd.NaT:\n",
            "        return np.nan\n",
            "    return date.year + date.month / 12.0 + date.day / (12.0 * monthrange(date.year, date.month)[1])\n",
            "\n",
            "\n",
            "def create_preprocessor(\n",
            "    keep_dates: bool = True,\n",
            "    keep_nums: bool = True,\n",
            "    keep_strs: bool = True,\n",
            "    encoding: str = \"one_hot\",\n",
            "    use_feat_aggl: bool = True,\n",
            ") -> Pipeline:\n",
            "    # Preprocessing of date features\n",
            "    date_transformer = Pipeline([\n",
            "        (\"converter\", FunctionTransformer(map_date)),\n",
            "        (\"imputer\", SimpleImputer()),\n",
            "        (\"scaler\", StandardScaler()),\n",
            "    ])\n",
            "\n",
            "    # Preprocessing of numeric features\n",
            "    num_transformer = Pipeline([\n",
            "        (\"imputer\", SimpleImputer()),\n",
            "        (\"scaler\", StandardScaler()),\n",
            "    ])\n",
            "\n",
            "    # Preprocessing of string (categorical) features\n",
            "    str_transformers = [\n",
            "        (\"imputer\", SimpleImputer(missing_values=pd.NA, strategy=\"constant\", keep_empty_features=True, fill_value=\"N/A\")),\n",
            "    ]\n",
            "    if encoding == \"one_hot\":\n",
            "        str_transformers.append((\"encoder\", OneHotEncoder(min_frequency=0.05, handle_unknown=\"infrequent_if_exist\", sparse_output=False)))\n",
            "    elif encoding == \"ordinal\":\n",
            "        str_transformers.append((\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)))\n",
            "    else:\n",
            "        raise Exception(\"unsupported encoding\")\n",
            "    str_transformer = Pipeline(str_transformers)\n",
            "\n",
            "    # Column transformer for the different feature data types\n",
            "    per_dtype_transformers = []\n",
            "    if keep_dates:\n",
            "        per_dtype_transformers.append(\\\n",
            "            (\"date\", date_transformer, make_column_selector(dtype_include=dtypes[\"date\"])))\n",
            "    if keep_nums:\n",
            "        per_dtype_transformers.append(\\\n",
            "            (\"num\", num_transformer, make_column_selector(dtype_include=dtypes[\"num\"])))\n",
            "    if keep_strs:\n",
            "        per_dtype_transformers.append(\\\n",
            "            (\"str\", str_transformer, make_column_selector(dtype_include=dtypes[\"str\"])))\n",
            "    \n",
            "    # Construction of the final pipeline\n",
            "    final_pipeline_transformers = [\n",
            "        (\"per_dtype\", ColumnTransformer(per_dtype_transformers)),\n",
            "        (\"variance_threshold\", VarianceThreshold()),\n",
            "    ]\n",
            "    if use_feat_aggl:\n",
            "        final_pipeline_transformers.append(\\\n",
            "            (\"feature_agglomeration\", FeatureAgglomeration(n_clusters=None, compute_full_tree=True, distance_threshold=10.0)))\n",
            "\n",
            "    return Pipeline(final_pipeline_transformers)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Inspect preprocessing pipeline\n",
            "Draw the preprocessing pipeline as a graph."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "create_preprocessor()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Visualize the preprocessed numerical features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preprocessor = create_preprocessor(keep_strs=False)\n",
            "preprocessor.set_output(transform=\"pandas\")\n",
            "X_train_prep: pd.DataFrame = preprocessor.fit_transform(X_train, y_train)\n",
            "X_train_prep.hist(bins=64, figsize=(60, 40))\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Look into the number of features after preprocessing and encoding"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preprocessor = create_preprocessor()\n",
            "preprocessor.set_output(transform=\"pandas\")\n",
            "X_train_prep: pd.DataFrame = preprocessor.fit_transform(X_train, y_train)\n",
            "X_train_prep.info()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Get the mapping of original features for feature agglomeration"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preprocessor_no_feat_aggl = create_preprocessor(use_feat_aggl=False)\n",
            "preprocessor_no_feat_aggl.set_output(transform=\"pandas\")\n",
            "X_train_prep_no_feat_aggl: pd.DataFrame = preprocessor_no_feat_aggl.fit_transform(X_train, y_train)\n",
            "\n",
            "for agg_idx in range(preprocessor[\"feature_agglomeration\"].n_clusters_):\n",
            "    line = f\"{agg_idx}: \"\n",
            "    for col in X_train_prep_no_feat_aggl.columns[preprocessor[\"feature_agglomeration\"].labels_ == agg_idx]:\n",
            "        line += f\"{col}, \"\n",
            "    print(line)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Model selection (training)\n",
            "This section contains the training and tuning of different models."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Helper functions"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Validates a model with a simple train/validate split.\n",
            "def validate_model(model: BaseEstimator):\n",
            "    X_val_train, X_val, y_val_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12) \n",
            "    model.fit(X_val_train, y_val_train)\n",
            "    y_pred = model.predict(X_val)\n",
            "    print(\"RMSE:\", root_mean_squared_error(y_val, y_pred))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Validates a model with 5-fold cross validation.\n",
            "def cross_validate_model(model: BaseEstimator):\n",
            "    scores = cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5, n_jobs=-1)\n",
            "    rmse_scores = np.sqrt(-scores)\n",
            "    print(\"RMSE Scores:\", rmse_scores)\n",
            "    print(\"RMSE Mean:\", rmse_scores.mean())\n",
            "    print(\"RMSE Std:\", rmse_scores.std())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Execute grid search with cross validation for a model and a given set of parameter combinations.\n",
            "def hyper_param_tune_model(model: BaseEstimator, param_grid):\n",
            "    grid_search = GridSearchCV(\n",
            "        estimator=model,\n",
            "        param_grid=param_grid,\n",
            "        scoring=\"neg_mean_squared_error\",\n",
            "        cv=5,\n",
            "        n_jobs=-1,\n",
            "        verbose=4,\n",
            "    )\n",
            "\n",
            "    grid_search.fit(X_train, y_train)\n",
            "\n",
            "    print(\"Results:\")\n",
            "    for params, rmse in zip(grid_search.cv_results_[\"params\"], np.sqrt(-grid_search.cv_results_[\"mean_test_score\"])):\n",
            "        print(f\"{params}:\\t {rmse}\")\n",
            "    print(\"Best Parameters:\", grid_search.best_params_)\n",
            "    print(\"Best RMSE:\", np.sqrt(-grid_search.best_score_))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Performe feature permutation importance to get each feature importance for a given model.\n",
            "def inspect_model_feature_importance(model: BaseEstimator, n_repeats: int = 5):\n",
            "    X_val_train, X_val, y_val_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12) \n",
            "    model.fit(X_val_train, y_val_train)\n",
            "\n",
            "    r = permutation_importance(model, X_val, y_val, n_repeats=n_repeats, n_jobs=-1, random_state=56)\n",
            "    for i in r.importances_mean.argsort()[::-1]:\n",
            "        if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
            "            print(f\"{X_train.columns[i]:<60} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\")\n",
            "    \n",
            "    return r"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Test a model with the test set.\n",
            "def test_model(model: BaseEstimator):\n",
            "    model.fit(X_train, y_train)\n",
            "    y_pred = model.predict(X_test)\n",
            "    print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Model training und tuning"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Linear Regression"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "linear_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"selector\", SelectKBest(f_regression, k=60)),\n",
            "    (\"regressor\", LinearRegression(n_jobs=-1)),\n",
            "])\n",
            "cross_validate_model(linear_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Random Forest Regressor"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.RandomState(42)\n",
            "rand_forest_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"selector\", SelectKBest(f_regression, k=60)),\n",
            "    (\"regressor\", RandomForestRegressor(n_jobs=-1, random_state=rng)),\n",
            "])\n",
            "cross_validate_model(rand_forest_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Histogram-Based Gradient Boosting Regressor"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.RandomState(42)\n",
            "hist_grad_boost_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", HistGradientBoostingRegressor(random_state=rng)),\n",
            "])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Hyper-parameter tuning"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_iter\": [50, 100, 200],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__learning_rate\": [0.05, 0.1, 0.2],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_leaf_nodes\": [16, 31, 62],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__min_samples_leaf\": [10, 20, 40],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_features\": [0.5, 0.8, 1.0],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_iter\": [50, 100, 200],\n",
            "    \"regressor__learning_rate\": [0.05, 0.1, 0.2],\n",
            "    \"regressor__max_leaf_nodes\": [16, 31, 62],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_iter\": [200, 400, 800],\n",
            "    \"regressor__learning_rate\": [0.2, 0.3, 0.4],\n",
            "    \"regressor__max_leaf_nodes\": [62, 80, 100],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "param_grid = {\n",
            "    \"regressor__max_iter\": [300, 350, 400],\n",
            "    \"regressor__learning_rate\": [0.15, 0.2, 0.25],\n",
            "    \"regressor__max_leaf_nodes\": [90, 100, 110],\n",
            "}\n",
            "\n",
            "hyper_param_tune_model(hist_grad_boost_reg, param_grid)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "hist_grad_boost_reg.set_params(**{\n",
            "    \"regressor__max_iter\": 400,\n",
            "    \"regressor__learning_rate\": 0.15,\n",
            "    \"regressor__max_leaf_nodes\": 90,\n",
            "})\n",
            "cross_validate_model(hist_grad_boost_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Try training the model with fewer features\n",
            "We are using `mutual_info_regression` from scikit-learn to reduce the number of features after preprocessing."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.RandomState(42)\n",
            "hist_grad_boost_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"selector\", SelectKBest(mutual_info_regression, k=100)),\n",
            "    (\"regressor\", HistGradientBoostingRegressor(max_iter=400, learning_rate=0.15, max_leaf_nodes=90, random_state=rng)),\n",
            "])\n",
            "cross_validate_model(hist_grad_boost_reg)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.RandomState(42)\n",
            "hist_grad_boost_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"selector\", SelectKBest(mutual_info_regression, k=20)),\n",
            "    (\"regressor\", HistGradientBoostingRegressor(max_iter=400, learning_rate=0.15, max_leaf_nodes=90, random_state=rng)),\n",
            "])\n",
            "cross_validate_model(hist_grad_boost_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Try training the model with fewer features based on feature permutation importance\n",
            "We use feature permutation importance to remove the number of features before the second preprocessing step."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.RandomState(42)\n",
            "hist_grad_boost_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", HistGradientBoostingRegressor(max_iter=400, learning_rate=0.15, max_leaf_nodes=90, random_state=rng)),\n",
            "])\n",
            "importance = inspect_model_feature_importance(hist_grad_boost_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Plotting some of the feature importances\n",
            "Plot the true LAID_UP_TIMEs against the predicted ones for specific features."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 45,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_val_train, X_val, y_val_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12) \n",
            "hist_grad_boost_reg.fit(X_val_train, y_val_train);"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "metadata": {},
         "outputs": [],
         "source": [
            "y_pred = hist_grad_boost_reg.predict(X_val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 47,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_feature(feature: str, X_val):\n",
            "    x_coords = X_val[feature][0::500]\n",
            "    y_coords_true = list(y_val[0::500])\n",
            "    y_coords_pred = list(y_pred[0::500])\n",
            "    plt.scatter(x=x_coords, y=y_coords_true, c=\"blue\", label=\"True samples\")\n",
            "    plt.scatter(x=x_coords, y=y_coords_pred, c=\"orange\", marker=\"+\", label=\"Predicted samples\")\n",
            "    for i, x in enumerate(x_coords):\n",
            "        if not pd.notna(x):\n",
            "            continue\n",
            "        if i == 0:\n",
            "            plt.plot([x, x], [y_coords_true[i], y_coords_pred[i]], c=\"red\", linestyle=\"--\", label=\"Residuals\")\n",
            "        else:\n",
            "            plt.plot([x, x], [y_coords_true[i], y_coords_pred[i]], c=\"red\", linestyle=\"--\")\n",
            "    plt.xlabel(feature)\n",
            "    plt.ylabel(\"LAID_UP_TIME\")\n",
            "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
            "    plt.legend()\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_feature(\"PURCHASE_DATE\", X_val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_feature(\"HORSEPOWER\", X_val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_val_shuffle: pd.DataFrame = X_val.copy()\n",
            "X_val_shuffle[\"PURCHASE_DATE\"] = np.random.permutation(X_val_shuffle[\"PURCHASE_DATE\"])\n",
            "y_pred = hist_grad_boost_reg.predict(X_val_shuffle)\n",
            "plot_feature(\"PURCHASE_DATE\", X_val_shuffle)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_val_shuffle: pd.DataFrame = X_val.copy()\n",
            "X_val_shuffle[\"HORSEPOWER\"] = np.random.permutation(X_val_shuffle[\"HORSEPOWER\"])\n",
            "y_pred = hist_grad_boost_reg.predict(X_val_shuffle)\n",
            "plot_feature(\"HORSEPOWER\", X_val_shuffle)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Cross validate final Histogram-Based Gradient Boosting Regressor"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sklearn\n",
            "sklearn.set_config(\n",
            "   assume_finite=True,  # disable validation\n",
            ")\n",
            "\n",
            "\n",
            "def perm_feat_importance_selector(X, y) -> np.array:\n",
            "    return importance.importances_mean\n",
            "\n",
            "\n",
            "rng = np.random.RandomState(42)\n",
            "selector = SelectKBest(perm_feat_importance_selector, k=50)\n",
            "selector.set_output(transform=\"pandas\")\n",
            "\n",
            "hist_grad_boost_reg = Pipeline([\n",
            "    (\"selector\", selector),\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", HistGradientBoostingRegressor(max_iter=400, learning_rate=0.15, max_leaf_nodes=90, random_state=rng)),\n",
            "])\n",
            "\n",
            "cross_validate_model(hist_grad_boost_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### LightGBM"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import lightgbm as lgb\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "\n",
            "\n",
            "rng = np.random.RandomState(42)\n",
            "\n",
            "param_grid = {\n",
            "    'regressor__boosting_type': ['dart'],\n",
            "    'regressor__learning_rate': [0.35],\n",
            "    # 'regressor__learning_rate': [0.1],\n",
            "    # 'regressor__n_estimators': [10000],\n",
            "    'regressor__n_estimators': [2000],\n",
            "    'regressor__num_leaves': [100],\n",
            "    # 'regressor__max_depth': [18, 19, 20],\n",
            "    'regressor__max_depth': [20],\n",
            "    'regressor__min_child_samples': [20],\n",
            "    'regressor__min_child_weight': [1e-2],\n",
            "    # 'regressor__min_split_gain': [0.0, 0.01, 0.1],\n",
            "    'regressor__subsample': [1.0],\n",
            "    'regressor__subsample_freq': [177],\n",
            "}\n",
            "\n",
            "lgbm_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", lgb.LGBMRegressor(random_state=rng, n_estimators=1000, n_jobs=28)),\n",
            "])\n",
            "\n",
            "grid_search = GridSearchCV(\n",
            "    estimator=lgbm_reg,\n",
            "    param_grid=param_grid,\n",
            "    # scoring='neg_mean_squared_error',\n",
            "    scoring='neg_root_mean_squared_error',\n",
            "    cv=5,\n",
            "    verbose=4\n",
            ")\n",
            "\n",
            "grid_search.fit(X_train, y_train)\n",
            "\n",
            "print(\"Best Parameters:\", grid_search.best_params_)\n",
            "print(\"Best RMSE:\", -grid_search.best_score_)\n",
            "\n",
            "# cross_validate_model(lgbm_reg)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import lightgbm as lgb\n",
            "\n",
            "rng = np.random.RandomState(42)\n",
            "\n",
            "lgbm_regressor = lgb.LGBMRegressor(\n",
            "        random_state=rng,\n",
            "        boosting_type=\"dart\",\n",
            "        n_estimators=5000,\n",
            "        learning_rate=0.35,\n",
            "        num_leaves=100,\n",
            "        max_depth=20,\n",
            "        min_child_samples=20,\n",
            "        min_child_weight=1e-2,\n",
            "        subsample=1.0,\n",
            "        subsample_freq=177\n",
            "        )\n",
            "\n",
            "lgbm_reg = Pipeline([\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", lgbm_regressor),\n",
            "])\n",
            "\n",
            "importance = inspect_model_feature_importance(lgbm_reg)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sklearn\n",
            "\n",
            "sklearn.set_config(\n",
            "    assume_finite=True,  # disable validation\n",
            ")\n",
            "\n",
            "\n",
            "def perm_feat_importance_selector(X, y) -> np.array:\n",
            "    return importance.importances_mean\n",
            "\n",
            "\n",
            "rng = np.random.RandomState(42)\n",
            "selector = SelectKBest(perm_feat_importance_selector, k=50)\n",
            "selector.set_output(transform=\"pandas\")\n",
            "\n",
            "lgbm_reg = Pipeline([\n",
            "    (\"selector\", selector),\n",
            "    (\"preprocessor\", create_preprocessor()),\n",
            "    (\"regressor\", lgbm_regressor),\n",
            "])\n",
            "\n",
            "cross_validate_model(lgbm_reg)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Select final model\n",
            "Select the final model based on the cross validation scores of each one."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 56,
         "metadata": {},
         "outputs": [],
         "source": [
            "final_model = hist_grad_boost_reg"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Evaluate final model on the test set\n",
            "Get an approximation for the generalization error of the final model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_model(final_model)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Predicting new data"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train final model on the full data set"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_prep = prepare_data_frame(df, is_test=False, drop_rows=True)\n",
            "X, y = split_data_frame(df_prep)\n",
            "final_model.fit(X, y);"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load and prepare test data frame"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_new = pd.read_excel(\"data/Vehicles_export_prices_scaled_stud_test_eng.xlsx\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_new.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 61,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_new_prep = prepare_data_frame(df_new, is_test=True, drop_rows=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_new_prep.info()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Predict new values"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 63,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_new, _ = split_data_frame(df_new_prep)\n",
            "y_new = final_model.predict(X_new)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Inspect predicted values"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.hist(y_new, bins=256)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save predicted values"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 65,
         "metadata": {},
         "outputs": [],
         "source": [
            "def save_to_xlsx(features: pd.DataFrame, predictions: np.array, filename: str):\n",
            "    output_dir = \"predictions\"\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "    \n",
            "    df_output = features.copy()\n",
            "    df_output[\"LAID_UP_TIME\"] = predictions\n",
            "    df_output[[\"CHASSIS_NUMBER\", \"LAID_UP_TIME\"]].to_excel(f\"{output_dir}/{filename}\", index = False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 66,
         "metadata": {},
         "outputs": [],
         "source": [
            "save_to_xlsx(X_new, y_new, \"teamA-model3.xlsx\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.13.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
